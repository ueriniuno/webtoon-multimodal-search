{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kr8r1_ox7DKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14c01bd7"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G2FF3vDn7TUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"transformers>=4.42.0\" \"accelerate>=0.26.0\" einops timm"
      ],
      "metadata": {
        "id": "939vy8TA67q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/microsoft/Florence-2-large\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/microsoft/Florence-2-large)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ğŸ™"
      ],
      "metadata": {
        "id": "goBcEfbT67q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ (ë²„ì „ ì¶©ëŒ ë°©ì§€)\n",
        "# !pip install -U transformers einops timm\n",
        "\n",
        "model_id = \"microsoft/Florence-2-large\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 2. ëª¨ë¸ ë¡œë“œ (ì—ëŸ¬ ìœ ë°œ ì¸ìë¥¼ ì œê±°í•˜ê³  ìµœì†Œí•œìœ¼ë¡œ ë¡œë“œ)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # SDPA ì—ëŸ¬ ë°©ì§€\n",
        ").to(device)\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë³„ë„ë¡œ half() ë³€í™˜\n",
        "# model = model.half()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")"
      ],
      "metadata": {
        "id": "zAuYIjYA9ZZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_florence2(image, task_prompt=\"<DETAILED_CAPTION>\"):\n",
        "    # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
        "    inputs = processor(text=task_prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 2. í…ìŠ¤íŠ¸ ìƒì„± (ì—ëŸ¬ ìˆ˜ì • í¬ì¸íŠ¸)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            num_beams=3,\n",
        "            do_sample=False,\n",
        "            # [ìˆ˜ì •] ì•„ë˜ ë‘ ì˜µì…˜ì´ ì—ëŸ¬ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
        "            use_cache=False,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # 3. ê²°ê³¼ ë””ì½”ë”© ë° í›„ì²˜ë¦¬\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(\n",
        "        generated_text,\n",
        "        task=task_prompt,\n",
        "        image_size=(image.width, image.height)\n",
        "    )\n",
        "\n",
        "    return parsed_answer[task_prompt]"
      ],
      "metadata": {
        "id": "7-0eyXKf67q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¯¸ì§€ ë¡œë“œ (ì—…ë¡œë“œí•œ íŒŒì¼ ê²½ë¡œ í˜¹ì€ URL)\n",
        "image_path = \"/content/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-12-28 á„‹á…©á„’á…® 5.14.07.png\"\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "# 1. ìƒì„¸ ë¬˜ì‚¬\n",
        "caption = run_florence2(img, task_prompt=\"<DETAILED_CAPTION>\")\n",
        "print(f\"ìƒì„¸ ë¬˜ì‚¬: {caption}\")\n",
        "\n",
        "# # # 2. ëŒ€ì‚¬ ì¶”ì¶œ (OCR) - ì›¹íˆ°ì˜ ë¬¸ë§¥ íŒŒì•…ì— ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "# ocr_text = run_florence2(img, task_prompt=\"<OCR>\")\n",
        "# print(f\"ì¶”ì¶œëœ ëŒ€ì‚¬: {ocr_text}\")"
      ],
      "metadata": {
        "id": "XkhUAWoK67q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8tpIJYt-UD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}