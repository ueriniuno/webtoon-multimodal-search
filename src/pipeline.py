# src/pipeline.py
import json
import os
from rank_bm25 import BM25Okapi
from qdrant_client.http import models

from src.config import settings
from src.schemas import SearchResult, ScenePayload
from src.router import LLMRouter
from src.expander import QueryExpander
from src.reranker import Reranker
from src.prompts import RAG_GENERATION, RAG_SYSTEM 
# üëá [ÏàòÏ†ï] KoreanTokenizer Ï∂îÍ∞Ä
from src.utils import load_json, KoreanTokenizer

class RAGPipeline:
    def __init__(self, db, embedding, llm):
        self.db = db
        self.embedding = embedding
        self.llm = llm
        
        print("üîß [Pipeline] Ïª¥Ìè¨ÎÑåÌä∏ Î°úÎî©...")
        
        self.lookup = load_json(settings.paths["lookup_store"])
        if not self.lookup:
            print("‚ö†Ô∏è [Warning] Lookup Store ÎπÑÏñ¥ÏûàÏùå.")
        
        # üëá [ÏàòÏ†ï] ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï¥àÍ∏∞Ìôî (Kiwi)
        self.tokenizer = KoreanTokenizer()

        # BM25 ÏóîÏßÑ Ï¥àÍ∏∞Ìôî
        self.bm25 = None
        self.bm25_data = [] 
        self._init_bm25()
        
        self.router = LLMRouter(llm)
        self.expander = QueryExpander(llm)
        self.reranker = Reranker()

    def _init_bm25(self):
        bm25_path = os.path.join(settings.paths["data_dir"], "bm25_corpus.json")
        if not os.path.exists(bm25_path):
            print("‚ö†Ô∏è [Warning] BM25 ÏΩîÌçºÏä§ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.")
            return
            
        print("   üìò BM25 Ïù∏Îç±Ïä§ ÏÉùÏÑ± Ï§ë (ÌòïÌÉúÏÜå Î∂ÑÏÑù Ï†ÅÏö©)...")
        with open(bm25_path, 'r', encoding='utf-8') as f:
            self.bm25_data = json.load(f)
        
        # üëá [ÏàòÏ†ï] split() ÎåÄÏã† ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ ÏÇ¨Ïö©
        tokenized_corpus = [self.tokenizer.tokenize(doc['text']) for doc in self.bm25_data]
        
        self.bm25 = BM25Okapi(tokenized_corpus)
        print("   ‚úÖ BM25 Ï§ÄÎπÑ ÏôÑÎ£å.")

    def hybrid_search(self, query, expanded_query, top_k=50):
        """
        [Vector Search] -> Expanded Query ÏÇ¨Ïö© (ÏùòÎØ∏ ÌååÏïÖ)
        [BM25 Search] -> Original + Expanded ÌòºÌï© ÏÇ¨Ïö© (ÏïàÏ†ÑÏÑ± + ÌôïÏû•ÏÑ±)
        """
        # 1. Vector Search
        query_vector = self.embedding.get_embeddings(expanded_query).tolist()
        vector_hits = self.db.search(query_vector=query_vector, limit=top_k)
        
        # 2. BM25 Search
        bm25_hits = []
        if self.bm25:
            # ÏõêÎ≥∏Í≥º ÌôïÏû•Îêú ÏøºÎ¶¨Î•º Ìï©Ï≥êÏÑú BM25Ïóê ÎÑ£ÏäµÎãàÎã§.
            combined_query = f"{query} {expanded_query}" 
            
            # üëá [ÏàòÏ†ï] Í≤ÄÏÉâÏñ¥ÎèÑ ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞Î°ú Ï™ºÍ∞¨ ("ÎèôÍµ¨Í∞Ä" -> ['ÎèôÍµ¨', 'Í∞Ä'])
            tokenized_query = self.tokenizer.tokenize(combined_query)
            
            doc_scores = self.bm25.get_scores(tokenized_query)
            top_indexes = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:top_k]
            
            for idx in top_indexes:
                hit_data = self.bm25_data[idx]
                bm25_hits.append(models.ScoredPoint(
                    id=hit_data['id'],
                    version=0,
                    score=doc_scores[idx],
                    payload=hit_data['payload'],
                    vector=None
                ))

        # 3. RRF (Reciprocal Rank Fusion)
        k = 60
        fused_scores = {}
        
        for rank, hit in enumerate(vector_hits):
            if hit.id not in fused_scores:
                fused_scores[hit.id] = {"score": 0, "payload": hit.payload, "obj": hit}
            fused_scores[hit.id]["score"] += 1 / (k + rank + 1)
            
        for rank, hit in enumerate(bm25_hits):
            if hit.id not in fused_scores:
                fused_scores[hit.id] = {"score": 0, "payload": hit.payload, "obj": hit}
            fused_scores[hit.id]["score"] += 1 / (k + rank + 1)
            
        sorted_ids = sorted(fused_scores.keys(), key=lambda x: fused_scores[x]["score"], reverse=True)
        
        final_results = []
        for pid in sorted_ids[:top_k]:
            item = fused_scores[pid]
            original_pt = item['obj']
            final_results.append(models.ScoredPoint(
                id=original_pt.id,
                version=0,
                score=item['score'],
                payload=item['payload'],
                vector=None
            ))
            
        return final_results

    def _fetch_window_context(self, points, window_size=0):
        if window_size <= 0:
            return {p.id: p.payload['text'] for p in points}

        ids_to_fetch = set()
        for p in points:
            center_id = p.id
            ids_to_fetch.add(center_id)
            for i in range(1, window_size + 1):
                ids_to_fetch.add(center_id - i)
                ids_to_fetch.add(center_id + i)

        fetched_points = self.db.client.retrieve(
            collection_name=settings.rag["collection_name"],
            ids=list(ids_to_fetch)
        )
        text_map = {fp.id: fp.payload['text'] for fp in fetched_points}

        merged_texts = {}
        for p in points:
            center_id = p.id
            full_text_list = []
            for i in range(window_size, 0, -1):
                t = text_map.get(center_id - i)
                if t: full_text_list.append(t)
            
            full_text_list.append(text_map.get(center_id, ""))
            
            for i in range(1, window_size + 1):
                t = text_map.get(center_id + i)
                if t: full_text_list.append(t)
            
            merged_texts[center_id] = " ".join(full_text_list)
            
        return merged_texts

    def _save_debug_log(self, query, expanded_query, scanned_points, final_docs):
        log_path = "debug_search_log.txt"
        with open(log_path, "w", encoding="utf-8") as f:
            f.write(f"=== Debug Log (Hybrid Search) ===\n")
            f.write(f"Original Query: {query}\n")
            f.write(f"Expanded Query: {expanded_query}\n\n")
            
            f.write(f"=== 1. Hybrid Retrieval Candidates (RRF Top {len(scanned_points)}) ===\n")
            for i, p in enumerate(scanned_points):
                f.write(f"[{i+1}] ID: {p.id} | RRF Score: {p.score:.6f}\n")
                f.write(f"    Text: {p.payload['text'][:100]}...\n")
            
            f.write(f"\n=== 2. Reranker Selected (Top {len(final_docs)}) ===\n")
            for i, doc in enumerate(final_docs):
                p = doc.payload
                f.write(f"[{i+1}] ID: {p.id} | Rerank Score: {doc.score:.4f}\n")
                f.write(f"    Full Context used:\n{doc.full_context_text[:200]}...\n\n")
        
        print(f"üìù [Log] Í≤ÄÏÉâ ÏÉÅÏÑ∏ ÎÇ¥Ïö©Ïù¥ '{log_path}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")

    def run(self, query, window_size=0):
        print(f"\nüöÄ [Pipeline] Ï≤òÎ¶¨ ÏãúÏûë: '{query}' (Hybrid Mode, Window: {window_size})")
        
        intent, cid = self.router.route(query)
        print(f"üö¶ [Router] Î∂ÑÏÑù Í≤∞Í≥º: Intent='{intent}', Chapter='{cid}'")
        
        if intent == "lookup_chapter" and cid:
            summary = self.lookup.get(f"chapter_{cid}", "Ï†ïÎ≥¥ ÏóÜÏùå")
            return self.llm.ask("ÎãπÏã†ÏùÄ ÏöîÏïΩ Î¥áÏûÖÎãàÎã§.", f"{cid}Ìôî ÎÇ¥Ïö©ÏùÑ ÏöîÏïΩÌï¥Ï§ò.\nÎÇ¥Ïö©: {summary}")

        search_query = self.expander.expand(query)
        print(f"üîç [Expander] ÌôïÏû•Îêú ÏøºÎ¶¨: '{search_query}'")

        scanned_points = self.hybrid_search(query, search_query, top_k=settings.rag["top_k_retrieve"])
        
        if not scanned_points: return "Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§."

        window_texts = self._fetch_window_context(scanned_points, window_size=window_size)

        candidates = []
        for hit in scanned_points:
            p = hit.payload
            center_id = hit.id
            
            extended_text = window_texts.get(center_id, p['text'])
            c_txt = self.lookup.get(f"chapter_{p['chapter_id']}", "")
            e_txt = self.lookup.get(f"event_{p.get('event_id')}", "")
            
            full_context_for_rerank = (
                f"{extended_text}\n\n"
                f"[Ï∞∏Í≥† - ÏÇ¨Í±¥: {e_txt}]\n"
                f"[Ï∞∏Í≥† - Ï†ÑÏ≤¥: {c_txt}]"
            )
            candidates.append(SearchResult(payload=ScenePayload(**p), full_context_text=full_context_for_rerank))

        final_docs = self.reranker.rerank(query=query, docs=candidates, top_k=settings.rag["top_k_final"])
        self._save_debug_log(query, search_query, scanned_points, final_docs)
        print(f"üéØ [Reranker] {len(scanned_points)}Í∞ú -> {len(final_docs)}Í∞ú ÏÑ†Ï†ï")

        events = set()
        chapters = set()
        scenes = []
        
        for doc in final_docs:
            p = doc.payload
            scenes.append(f"- [{p.chapter_id}Ìôî {p.scene_idx}Ïª∑] {p.text}")
            
            c_full = self.lookup.get(f"chapter_{p.chapter_id}", "")
            if c_full: chapters.add(f"- [Ch.{p.chapter_id}] {c_full}")
            if p.event_id:
                e_full = self.lookup.get(f"event_{p.event_id}", "")
                if e_full: events.add(f"- [Event] {e_full}")

        final_prompt = RAG_GENERATION.format(
            # ‚òÖ [ÏàòÏ†ï] query -> search_queryÎ°ú Î≥ÄÍ≤Ω! (ÌôïÏû•Îêú ÏøºÎ¶¨Î•º ÎÑòÍ≤®Ïïº Ïù¥Î¶Ñ Îß§Ïπ≠Ïù¥ Ïûò Îê®)
            character_info=self.expander.get_profile_str(search_query),
            global_summary=self.lookup.get("global", ""),
            context_summaries="\n".join(events) + "\n" + "\n".join(chapters),
            scene_details="\n".join(scenes),
            user_query=query
        )

        return self.llm.ask(RAG_SYSTEM, final_prompt)