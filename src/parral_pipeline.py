# src/pipeline.py
from qdrant_client.http import models

class RAGPipeline:
    def __init__(self, db, embedding, llm):
        self.db = db
        self.embedding = embedding
        self.llm = llm

    def retrieve(self, query, top_k_chapters=3, top_k_anchors=5, window_size=0):
        """
        [ë³‘ë ¬ ê²€ìƒ‰ ë¡œì§]
        1. ì±•í„° ê²€ìƒ‰: ì¤„ê±°ë¦¬ ë§¥ë½(Global Context) í™•ë³´ìš© (ì”¬ ê²€ìƒ‰ í•„í„°ë§ X)
        2. ì”¬ ê²€ìƒ‰: ì „ì²´ DB ëŒ€ìƒ ë…ë¦½ì  ê²€ìƒ‰ (Local Context)
        3. ë°˜í™˜: (ì”¬_ë¦¬ìŠ¤íŠ¸, ìš”ì•½ë³¸_ë¦¬ìŠ¤íŠ¸)
        """
        query_vector = self.embedding.get_embeddings(query).tolist()

        # --- Step 1: ì±•í„° ê²€ìƒ‰ (ë…ë¦½ì  ìˆ˜í–‰) ---
        print(f"ğŸ“¡ [Step 1] ì±•í„°(Global Context) ê²€ìƒ‰ (Top-{top_k_chapters})...")
        
        chapters = self.db.client.query_points(
            collection_name=self.db.chapter_col,
            query=query_vector,
            limit=top_k_chapters
        ).points
        
        relevant_summaries = []
        
        if not chapters:
            print("   âš ï¸ ê´€ë ¨ ì±•í„° ì—†ìŒ")
        else:
            print(f"   ğŸ” í™•ë³´ëœ ì¤„ê±°ë¦¬ ë§¥ë½:")
            for c in chapters:
                ch_id = c.payload['chapter_id']
                score = c.score
                
                # LLMì—ê²Œ ì¤„ ìš”ì•½ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                summary_text = f"[Chapter {ch_id} ìš”ì•½] {c.payload['summary']}"
                relevant_summaries.append(summary_text)
                
                print(f"     - Ch.{ch_id} (ìœ ì‚¬ë„: {score:.4f})")

        # --- Step 2: ì”¬ ê²€ìƒ‰ (ì „ì²´ ë²”ìœ„ ëŒ€ìƒ ìˆ˜í–‰) ---
        # [ë³€ê²½ì ] ì±•í„° IDë¡œ í•„í„°ë§(query_filter)ì„ ê±¸ì§€ ì•ŠìŠµë‹ˆë‹¤!
        print(f"ğŸ“¡ [Step 2] ì”¬(Local Context) ì „ì²´ ê²€ìƒ‰ (Top-{top_k_anchors})...")

        scenes = self.db.client.query_points(
            collection_name=self.db.scene_col,
            query=query_vector,
            query_filter=None,  # ğŸ‘ˆ í•µì‹¬: í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰
            limit=top_k_anchors
        ).points

        # --- Step 3: ìœˆë„ìš° í™•ì¥ ---
        final_scene_ids = set()
        
        for anchor in scenes:
            ch_id = anchor.payload['chapter_id']
            center_idx = anchor.payload['scene_idx']
            
            # window_size ë§Œí¼ ì•ë’¤ í™•ì¥
            for i in range(center_idx - window_size, center_idx + window_size + 1):
                if i < 1: continue 
                expanded_id = ch_id * 10000 + i
                final_scene_ids.add(expanded_id)

        if not final_scene_ids:
            return [], relevant_summaries # ì”¬ì€ ì—†ì–´ë„ ìš”ì•½ì€ ë°˜í™˜

        # --- Step 4: ìµœì¢… ë°ì´í„° ì¡°íšŒ ---
        print(f"ğŸ“¡ [Step 3] ìµœì¢… {len(final_scene_ids)}ê°œ ì»· ë°ì´í„° ë¡œë”© (Window: +/-{window_size})")
        retrieved_points = self.db.client.retrieve(
            collection_name=self.db.scene_col,
            ids=list(final_scene_ids)
        )
        
        final_docs = sorted(retrieved_points, key=lambda x: (x.payload['chapter_id'], x.payload['scene_idx']))
        
        return final_docs, relevant_summaries

    def generate_answer(self, query, results_tuple):
        """
        results_tuple: (final_docs, summaries)
        """
        documents, summaries = results_tuple
        
        print(f"âœï¸ [Generation] ë‹µë³€ ìƒì„± ì¤‘ (ìš”ì•½ë³¸ {len(summaries)}ê°œ + ì”¬ {len(documents)}ê°œ ì°¸ê³ )...")
        
        # 1. Global Context
        summary_context = "\n".join(summaries)
        
        # 2. Local Context
        scene_context_list = []
        for doc in documents:
            p = doc.payload
            scene_desc = f"[{p['chapter_id']}í™” {p['scene_idx']}ì»·] {p['full_text']}"
            scene_context_list.append(scene_desc)
        
        full_scene_context = "\n\n".join(scene_context_list)
        
        system_msg = "ë‹¹ì‹ ì€ ì›¹íˆ° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. [ì „ì²´ ì¤„ê±°ë¦¬]ì™€ [ìƒì„¸ ì¥ë©´]ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”."
        
        user_msg = f"""
### 1. ì „ì²´ ì¤„ê±°ë¦¬ ë§¥ë½ (Global Context):
{summary_context}

### 2. ìƒì„¸ ì¥ë©´ ë§¥ë½ (Local Context):
{full_scene_context}

### ì§ˆë¬¸:
{query}

### ë‹µë³€:
"""
        return self.llm.ask(system_msg, user_msg)