import os
import json
import re  # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€ (íŒŒì¼ëª… ë¶„ì„ìš©)
import torch
import easyocr
from PIL import Image
from tqdm import tqdm
from transformers import BlipProcessor, BlipForConditionalGeneration
from config import IMAGE_DIR, METADATA_FILE

def run_preprocessing():
    # 1. ì´ë¯¸ì§€ í´ë” í™•ì¸
    if not os.path.exists(IMAGE_DIR):
        print(f"âŒ '{IMAGE_DIR}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸš€ Starting Pre-processing (OCR + Captioning)...")
    
    # 2. ëª¨ë¸ ë¡œë“œ (OCR & VLM)
    print("â³ Loading AI Models...")
    reader = easyocr.Reader(['ko', 'en']) # í•œêµ­ì–´, ì˜ì–´ ì¸ì‹
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    
    # GPU ê°€ì† ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"âœ… Models Loaded on {device}")

    # 3. ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
    dataset = []

    print(f"ğŸ“‚ Found {len(image_files)} images. Processing start!")

    # 4. ë°˜ë³µë¬¸ ì‹œì‘ (ì´ë¯¸ì§€ í•˜ë‚˜ì”© ì²˜ë¦¬)
    for img_file in tqdm(image_files):
        try:
            img_path = os.path.join(IMAGE_DIR, img_file)
            
            # --- [í•µì‹¬ ì¶”ê°€ ê¸°ëŠ¥] íŒŒì¼ëª…ì—ì„œ ì—í”¼ì†Œë“œ/ì»· ì •ë³´ ì¶”ì¶œ ---
            # ê·œì¹™: epìˆ«ì_ìˆ«ì (ì˜ˆ: ep01_024.jpg -> 1í™”, 24ì»·)
            episode = -1
            scene_no = -1
            
            # ì •ê·œì‹: "ep" ë’¤ì— ìˆ«ì + "_" + ìˆ«ì íŒ¨í„´ ì°¾ê¸°
            match = re.search(r'ep(\d+)[_](\d+)', img_file.lower())
            if match:
                episode = int(match.group(1))
                scene_no = int(match.group(2))
            # ----------------------------------------------------

            # 5. OCR (ê¸€ì ì¶”ì¶œ)
            # detail=0 ì€ ì¢Œí‘œ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            ocr_result = reader.readtext(img_path, detail=0)
            ocr_text = " ".join(ocr_result)

            # 6. Captioning (ì¥ë©´ ë¬˜ì‚¬ ìƒì„±)
            raw_image = Image.open(img_path).convert('RGB')
            inputs = processor(raw_image, return_tensors="pt").to(device)
            
            # ìº¡ì…˜ ìƒì„± (max_new_tokens=50 ì •ë„ë¡œ ê¸¸ì´ ì œí•œ)
            out = model.generate(**inputs, max_new_tokens=50)
            caption = processor.decode(out[0], skip_special_tokens=True)

            # 7. ë°ì´í„°ì…‹ì— ì¶”ê°€
            dataset.append({
                "scene_id": img_file,       # íŒŒì¼ëª… (ID ì—­í• )
                "episode": episode,         # ëª‡ í™” (í•„í„°ë§ìš©)
                "scene_no": scene_no,       # ëª‡ ì»· (ì •ë ¬ìš©)
                "image_path": img_path,     # íŒŒì¼ ê²½ë¡œ
                "scene_summary": caption,   # AIê°€ ë³¸ ì¥ë©´ ì„¤ëª…
                "ocr_text": ocr_text        # ë§í’ì„  ë‚´ìš©
            })

        except Exception as e:
            print(f"âš ï¸ Skipping {img_file}: {e}")

    # 8. ê²°ê³¼ ì €ì¥ (JSON)
    with open(METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, indent=4, ensure_ascii=False)
    
    print(f"âœ… Saved metadata to {METADATA_FILE}")
    print(f"ğŸ‰ Total {len(dataset)} items processed.")

if __name__ == "__main__":
    run_preprocessing()